In the last video, you saw how to evaluate an output in an example where it had the right answer. And so we could write down the function to explicitly just tell us if the element outputs the right categories and list of products. But one of the LLM is used to generate text data. Is it just one right piece of text? Let's take a look at an approach for how to evaluate that type of o m outputs.

Here's my usual helper functions and and given a customer message Tell me about the Smart X Pro Phone and the full stack camera and so on. Here are a few utils to get me the assistant answer. This is basically the process that EASER has stepped through in earlier videos, and so here's the assistant answer. Sure. We have a help.

smartphone, the Smart X Pro Phone, and so on and so forth. So how can you evaluate if this is a good answer or not? Seems like there are lots of possible good answers. One way to evaluate this is to write a rule break, meaning a set of guidelines to evaluate to this answer on different dimensions, and then use that to decide whether or not you're satisfied with this answer. Let me show you how to do that.

So let me create a little data structure to store the custom message as well as the product info. So here, I'm going to specify a prompt for evaluating the assistant answer using what's called a rubric. I'll explain in a second what that means. But this prompt says, in the system message, you're an assistant that validates how well the customer service agent answers the user question, but look in the context, that the customer service agent is using to generate response. So this response is what we had further up the notebook.

That was the assistant answer. and we're going to specify the data in this prompt, was the customer message, was the context that is was the product and category information that's provided? And then what was the output to the LM? And then this is a rubric. So we want the LM to compare the factory content as soon as it is content or even difference in the style of grammar, punctuation.

And then we wanted to check a few things like is the assistant response based only on the context provided? Does the answer include information that is not provided in the context? Is there any disagreement between response and the context? So this is called a rule break, and this specifies what we think the answer should get right for us to consider as a good answer. Then finally, we wanted to print out yes or no and so on.

And now, if we were to run this evaluation, this is what you get. So it says, the assistant response is based on the accounts provided. It does not, in this case, seem to make up new information There isn't disagreements. User asked 2 questions, answer question 1, and answer question 2. So answer both questions.

So we would look at this output and maybe conclude that this is a pretty good response. And one note here, I'm using the chat GBD 3.5 Turbo model for this evaluation. For a more robust evaluation, it might be worth considering using 4 because even if you deploy 3.5 Turbo in production and general longer text if your evaluation is a more sporadic exercise, then it may be prudent to pay for the somewhat more expensive GPT 4 API call. to get a more rigorous evaluation of the outputs of one design pattern that I hope you take away from this is that way you can specify a rubric. meaning a list of criteria by which to evaluate an alarm output, then you can actually use another API call to evaluate your first output.

There's one of the design pattern that could be useful for some applications. which is if you can specify an ideal response. So here, I'm going to specify a test example where the customer message is, tell me about the Smart X Profile and so on, and here's an ideal answer. So this is if you have a expert human customer service representative write a really good answer. And the expert says, this would be a great answer.

Of course, the Smiles profile would be so on. and goes on to give a lot of helpful information. Now it is unreasonable to expect LLM to generate this exact answer word for words. And in classical natural language processing techniques, there are some traditional metrics for measuring if the LMM output is similar to this expert human written output. For example, there's something called the blue score BLEU that you can search online to read more about, they can measure how similar one piece of text is from another.

But it turns out there's an even better way, which is you can use a prompt which I'm gonna specify here to ask an LLM to compare how well the automatically generated customer service agent outputs corresponds to the ideal expert response that was written by a human that I just showed up above. Here's the prompt we can use, which is we're going to use an LLM and tell it to be an assistant, the evaluance, how will the customer service agents asked to use a question by comparing the response that was the automatically generated 1 to the ideal expert human written response. And so we're going to give it the data, which is was the customer request. Once the expert written ID response, and then what did ROM actually outputs. And this rubric comes from the open AI, open source EVAS framework.

which is a fantastic framework with many evaluation methods contributed both by OpenAI Developers and by the broader open source community. In fact, if you want, you could contribute and eval to that framework yourself to help others evaluate their launch language model outputs. So in this rubric, we tell the LLM to compare the factual content and the submitted answer of the expert answer. a lot difference in the style of gram or punctuation, and feel free to pause the video and read through this in detail. But the key is we ask it to carry the comparison and outputs a score from a to e depending on whether the submitted answer is a subset to the expert answer and it's fully consistent.

versus the summit answer is a superset of the expert answer, but it's fully consistent with it. This might mean it hallucinated or made up some additional facts. 7th answer contains all the details as a expert answer whether it's disagreement, or whether the answers differ, but these differences don't matter from the perspective of factuality. And the LM will pick whichever of these seems to be the most appropriate description. So here's the assistant answer that we had just now.

I think it's a pretty good answer, but now let's see what the things when it compares the assistant answer to test set ID. Looks like it got an a. And so it thinks the submittent answer is a subset of the expert answer. and it's fully consistent with it. And that sounds right to me.

This assistant answer is much shorter than the long split answer up top, but it does, hopefully, is consistent. Once again, I'm using gpt3.5turbo in this example, but to get a more rigorous evaluation, this it might make sense to use GIP default in your own application. Now let's try something totally different. I'm going to have a very different assistant answer. Life is like a box of chocolate called from a movie called Forrest Gump.

And if we were to evaluate that, it outputs d, and it concludes that there's a disagreement between submitted answer, life is like a box of chocolate and the expert answer. So it correctly assesses this to be a pretty terrible answer. And so that's it. I hope you take away from this video to design patterns. First is even without an expert provided ideal answer.

If you can write a rubric, you can use LLM to evaluate another output. And second, if you can provide an expert provided ID answer, then that can help your LLM better compare if and if a specific assistant output is similar to the expert provided ID answer. I hope that helps you to evaluate your LM systems outputs so that both during development as well as when the system's running and you're getting responses, you can continue to monitor its performance and also have these tools to continuously evaluate and keep on improving the performance of your system.