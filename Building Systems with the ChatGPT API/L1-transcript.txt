Speaker 0:

In this first video, I'd like to share with you an overview of how LLM's large language models work. We'll go into how they are trained as well as details like the tokenizer and how that can affect the outputs of when you prompt an LLM. And we'll also take a look at the chat format for LLMs, which is a way of specifying both system as well as user messages and understand what you can do with that capability. Let's take a look. First, how does a large language model work?

You're probably familiar with the text generation process where you can give a prompt. I love eating and ask an LLM to go in what the things are likely completions given this prompt, and it may say, Vegas of cream cheese or my mother's meat loaf or off to a friends. But how did the model learn to do this? Demet who used to train an LLM is actually supervised learning. In supervised learning, a computer learns an input, outputs, or x to y mapping using labeled training data.

So, for example, ever using supervised learning to learn to classify the sentiment of restaurant reviews, you might collect a trading set like this. where a review like the past driving time, which is great, is labeled as a positive sentiment review and so on. And Service slow the fuel source was negative, and the error rate here was fantastic as a positive label. By the way, both Ezer and I we're born in the UK, and so both of us like our or gray tea. And so their process for supervised learning is typically to get labeled data and then train the model on data.

And after training, you can then deploy and call the model and give it a new restaurant review like best pizza I've have had you would hopefully output that that has a positive sentiment. It turns out that supervised learning is a core building block for training large language models. Specifically, a launch language model can be built by using supervised learning to repeatedly predict the next word. Let's say that In your training sets of a lot of text data, you have to sentence my favorite food is a bagel of cream cheese and mocks. then this sentence is turned into a sequence of training examples where given a sentence fragment, my favorite fruit is a.

if you want to predict the next word in this case was bagel, or given the sentence fragment or sentence prefix, my favorite food is a bagel. the next word, in this case, would be with and so on. And given the last training sets of 100 of 1,000,000,000 or sometimes even more words, you can then create a massive training set where you can start off with part of a sentence or part of a piece of text. and repeatedly ask the language model to learn to predict what is the next word. So today, there are broadly 2 major types of large language models.

The first is a base OM. And the second, which is what is increasingly used is the instruction to know So the base OM repeatedly predicts the next word based on text training data. And so if I give it a prompt, once upon a time, there was unicorn, then it may by repeatedly predicting one word at a time come up with a completion that tells a story about their unicorn living in a magical forest with our unicorn friends. Now a downside of this is that if you were to prompt it with what is a capital of France, quite plausible that on the Internet, there might be a list of questions about France. So I may complete this with what is France's largest city, what is France's population, and so on.

But what you really want is you wanted to tell you what is the capital of France probably rather than list all these questions. So the instructions you know, instead tries to follow instructions and will hopefully say the couple of Francis Paris. How do you go from a base LM to an instruction tuned LM? This is what the process of training an instruction tuned LM like chat GPC looks like. you first train a base LLM on a lot of data, so 100 of 1,000,000,000 of words, maybe even more.

And this is a process that can take months on a large supercomputing system. After you've trained the base LLM, you would then further train the model by fine tuning it on a smaller set of examples where the output follows an input instruction. And so for example, you may have contractors. Hope you write a lot of examples of an instruction, and then a good response to an instruction and that creates a training set to carry out this additional fine tuning. So the learners predict, what is the next word if it's trying to follow an instruction.

After that, to improve the quality of the l m's output, a common process now is to obtain human ratings of the quality of many different OM outputs on criteria, such as whether the output is helpful, honest, and harmless. And you can then further tune the LM to increase the probability of its generating the more highly rated outputs. And the most common technique to do this is RoHF, which stands for reinforcement learning from human feedback. And whereas training the base L M can take months, the process of going from the base L M to the instruction to L M can be done in maybe days on much more on a much more modest sized datasets and much more modest sized computational resources. So this is how you would use an OM on gonna import a few libraries.

I'm going to load my open AI key here. I'll say a little bit more about this later in this video, and here's a helper function to get a completion given a prompt. If you have not yet installed the OpenAI package on your computer, you might have to run PIP install OpenAI. but I already have an install here. So I will run that.

And let me hit Shaventa to run these, and now I can set response equals Get completion? What is the capital of France? And, hopefully, it will give me a good result. Now in the description of the large language model so far, I talked about it as predicting one word at a time. there's actually one more important technical detail.

If you were to tell it, take the letters in the word word lollipop, and reverse them. This seems like an easy task. Maybe like a four year old could do this task. But if you were to ask try GPT to do this, it actually outputs a somewhat garbled whatever this is. This is not l o l i p o p.

This is not lollipop's letters reverse. So why is Chad GPT unable to what seems like a relatively simple task. It turns out that there's one more important detail for how a large language model works, which is It doesn't actually repeatedly predict the next words. It instead it repeatedly predicts the next token. And what that Elm actually does is it will take a sequence of characters like learning new things as fun and group the characters together to form tokens that comprise commonly occurring sequences of characters.

So here, learning new things is fun. Each of them is a fairly common word. And so each token corresponds to one word or one word in a space or an exclamation mark. But if you were to give it input with some somewhat less frequently used words, like prompting us powerful developer to, the word prompting is still not that common in the English language that certainly gave me a popularity. And so prompting is actually broken down to 3 tokens with prompt and ing because those 3 are commonly occurring sequences of letters.

And if you were to give it the word lollipop, the tokenizer actually breaks us down into 3 tokens, l and o, and e pop. And because Charge GPT isn't seeing the individual letters, is instead seeing these 3 tokens, it's more difficult for it to correctly print out these letters in reverse order. So here's a trick you can use to fix this. If I were to add dashes, between these letters, and spaces would work too or other things would work too. Until table letters and lobby proper reverse them, then it actually does a much better job, this LOI POP.

And the reason for that is if you pass it lollipop with dashes in between the letters, it tokenizes each of these characters into an individual token, making it easier for it to see the individual letters and print them out in reverse order. So if you ever want to use a chat GPU to play a word game, like, Wordle or scrapple or something, this nifty trick helps it to better see the individual letters of the words. For the English language, one token roughly on average, corresponds to about four characters or about three quarters of a word. And so different large language models will often have different limits on the number of input plus output tokens it can accept. The input is often called the contact and the output is often called the completion.

And the model, GPT 3.5 Terrible, for example, is the most commonly used chat GPT model. has a limit of roughly 4000 tokens in the inputs plus output. So if you try to feed in an input context that's much longer than this, so actually throw an exceptional generating error. Next, I wanna share with you another powerful way to use NOM API, which involves specifying separate system, user, and assistance messages. Let me show you an example, then we'll we can explain in more detail what is actually doing.

Here's a new helper function called get completion from messages. And when we prompt as OM, we are going to give it multiple messages. Here's an example of what you can do. I'm going to specify first a message in the row of a system, so this a system message and the content of the system messages, you're in the system who responds to the style of Doctor Seuss. then I'm gonna specify a user message.

So the row of the second message is row user, and the content of this is write me a very short poem about a happy carrot. And so let's run that. And with temperature equals 1, I actually never know what's gonna come on. But, okay, that's a cool point. Oh, how Jolley is this carrot that I see?

And then, actually, rhymes pretty well. Alright. Well done. Chat GPD. And so in this example, the system message specifies the overall tone of what do you want the large language model to do?

And the user message is a specific instruction that you wanted to carry out given this higher level behavior that was specified in the system message. Here's a illustration of how it all works. So this is how the chat format works. The system message says the overall tonal behavior of the launch language model or the assistant. And then when you give it a user message such as maybe such as a tell me a joke or write me a poem, it will then output an appropriate response following what you asked for and to use the message and consistent with the overall behavior set in the system message.

And by the way, although I'm not illustrating it here, if you want to use this in a multi turn conversation, you can also input assistant messages. In this messages format, to let Chargebee know what it had previously said. if you wanted to continue the conversation based on things that had previously said as well. But here are a few more examples. if you want to set the tone to tell it to have a one sentence long output, then in the system message.

I can say all your responses must be one sentence long. And when I execute this, it outputs a single sentence is no longer a poem, not in a star Doctor Seuss, but it's a single sentence. There's a story about the happy carrots. And if we want to combine both specifying the style and the length, then I can use the system message to say, you're in the system in response to South Doctor Seuss. All your sentences must be one sentence long.

And now This generates a nice one sentence poem. It was always small and never scary. I like that. That's a very happy poem. And then lastly, just a fun.

If you are using an OM and you want to know how many tokens are you using. Here's a whole per function that is a little bit more sophisticated in that it gets a response from the open AI API endpoint, and then it uses other values in the response to tell you how many prompt tokens, completion tokens, and total tokens were used in your API call. Let me define that And if I run this now, here's a response, and here is Accounts of how many tokens we use. So this output, which had 55 tokens, whereas the prompt input had 37 tokens of this used up 92 tokens altogether. when I'm using our models in practice, I don't worry that much, frankly, about the number of tokens I'm using.

Maybe one case where it might be worth checking the number of tokens is if you're aware that the user might have given you too long in inputs that exceeds the 4000 also token limit of Charge GPT. In which case, you could double check how many tokens it was and truncated to make sure you're staying within the input token limit. of the large language model. Now I want to share with you one more tip for how to use a large language model. calling the open AI API requires using an API key that's tied to either a free or a paid account.

And so many developers will write the API key in plain text like this into the Jupyter Notebook. And this is a less secure way of using API keys that I would not recommend you use because it's just too easy to share this notebook with someone else or check this into GitHub or something and does end up leaking your API key to someone else. In contrast, what you saw me do in the Jupyter Notebook was this piece of codes where I use a library dot nth, and then run this command, load dot n, find dot nth, to read a local file, which is called dot env that contains my secret key. And so with this code snippet, I have locally stored a file called dot nth that contains my API key, and this loads it into the operating systems, environmental variable, and then OS dot get nth. Open their API keys stores it into this variable.

And in this whole process, I don't ever have to enter the API key in plain text and unencrypted plain text into, like, you for the notebook. So this is a relatively more secure and a better way to access the API team. And in fact, this is a general method for storing different API keys from loss of different online services that you might want to use in call from your Jupyter notebook. Lastly, I think the degree to which prompting is revolutionizing AI application development is still underappreciated. In the traditional supervised machine learning workflow, like the restaurant review sent to a classification example that I touched on just now, If you want to build a classifier, to classify restaurant review, possible negative sentiments, you at first get bunch of labeled data maybe hundreds of examples.

This might take, I don't know, weeks, maybe a month. Then you would train a model on data and getting an appropriate open source model tuning on the model, evaluating it, that might take days, weeks, maybe even a few months. And then you might have to find a cloud service to deploy it, and then get your model uploaded to the cloud, and then run the model, and finally, be able to call your model. And it's, again, not uncommon. for this to take a team a few months to get working.

In contrast, with prompting base machine learning, When you have a text application, you can specify a prompt. This can take minutes, maybe hours if you need to iterate a few times to get an effective prompt. And then in hours, maybe at most days, but frankly, more often, hours, you can have this running using API calls and start making calls the model. And once you've done that in just again, maybe minutes or hours, you can start calling the model and start making inferences. And so there are applications that used to take me maybe 6 months or a year to build that you can now build in minutes or hours, maybe very small numbers of days using prompting.

And this is revolutionizing what AI applications can be built quickly. One important caveat, this applies to many unstructured data applications, including specifically text applications and maybe increasingly vision applications, although the vision technology is much less mature right now, but it's kinda getting there. This recipe doesn't really work. for structured data applications, meaning machine learning applications on tabular data with lots of numerical values and Excel spreadsheet, say, But for applications to which this does apply, the fact that AI components can be built so quickly is changing the workflow of how the entire system might be built. Building the entire system might still take days or weeks or something, but at least this piece of it can be done much faster.

And so with that, let's go on to the next video where EASER will show how to use these components to evaluate the input to a customer service assistant, and this will be part of a bigger example that you see developed through this course. for building a customer service assistance for an online retailer.