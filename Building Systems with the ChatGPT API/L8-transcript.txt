Speaker 0:

In the previous few videos, Isa showed how to build an application using an LLM from evaluating the inputs to processing the inputs to then doing final output checking before you show the outputs. to a user. After you built such a system, how do you know how it's working? And maybe even as you deploy it and let users use it, how can you track how it's doing and find any shortcomings and continue to improve the quality of the answers in the system. In this video, I'd like to share with you some best practices for evaluating the output of an LLM.

And I want to share with you specifically what it feels like to build one of these systems. One key distinction between what you hear me talk about in this video and what you may have seen in more traditional machine learning, supervised learning applications is because you can build such an application so quickly, the methods are evaluating it. It tends not to start off with a test set. Instead, you often end up gradually building up a set of test examples. Let me share with you what I mean by that.

You will remember this diagram some from the second video about how prompt based development speeds up the core parts of model development from maybe months to just minutes or hours or at most a very small number of days. In the traditional Supai's learning approach, if you needed to collect, say, 10,000 labeled examples anyway, then the incremental cost of collecting another 1000 test examples isn't that bad. So in the traditional supervised learning setting, it was not unusual to collect a training set, collect a development sets or hold our cost validation set in the test set and then tap those at hand throughout this development process. But if you're able to specify a prompt in just minutes and get something working in hours, then it would seem like a huge pain if you had to pause for a long time to collect a 1000 test examples because you cannot get this working with 0 training examples. So when building an application using an LLM, this is what it often feels like.

First, you would tune the prompts on just a small handful of examples, maybe 1 to 3 to 5 examples, and try to get a prompt that works on them. And then as you have the system undergo additional testing, you occasionally run into a few examples that tricky. The prompt doesn't work on them or the algorithm doesn't work on them. And in that case, you can take these additional 1 or 2 or 3 or 5 examples and add them to the set that you're testing on to just add additional tricky examples opportunistically. Eventually, you have enough of these samples you've added to your slowly growing development set, then it becomes a bit inconvenient to manually run every example through the prompt every time you change the prompt, and then you start to develop metrics to measure performance on this small set of examples, such as maybe average receipt.

And one interesting aspect of this process is if you decide at any moment in time, your system is working well enough, you can stop right there and not go on to the next bullet. And in fact, there are many deploy applications that stops at maybe the first or the second bullet and are running actually and and are running just fine. Now, if your hand built development set that you're evaluating the model on isn't giving you sufficient confidence yet in the performance of your system, then that's when you may go to the next step of of collecting a randomly sample set of examples to tune the model to. And this would continue to be a development set or a whole lot of cross validation set. It could be quite because it'd be quite common to continue to tune your prompt to this.

And only if you need even higher fidelity estimate of the performance of your system, then might you collect and use a hold on test sets that know, you don't even look at yourself when you're tuning the model. And so step 4, tends to be more important if, say, your system is getting the right answer 91% of the time and you want to tune it to get it to give the right answer 92 or 93 percent of the time, then you do need a larger set of examples to measure those differences between 91 and 93% performance. And then only if you really need a unbuyers fair estimate of how was the system doing, then do you need to go beyond the development set to also collect a holdout test set? One important caveat, I've seen a lot of applications of large language models where there isn't meaningful risk of harm, if it gives not quite the right answer. But, obviously, for any high stakes applications, if there's a risk of bias or an inappropriate output causing harm to someone, then the responsibility to collect a test set to rigorously evaluate the performance of your system to make sure it's doing the right thing before you use it, that becomes much more important.

But if, for example, if you are using it to summarize articles just for yourself to read and no one else, then maybe the risk of harm is more modest. and you can stop early in this process without going to the expense of bullets 45 in collecting larger datasets on which they value your algorithm. So in this example, let me start with the usual helper functions. Use a utility function to get a list of products and categories. So in the computers and laptop category.

There's a list of computers and laptops in the smartphone's accessory category. Here's a list of smartphones and accessories and so on for other categories. Now, Let's say, the task of an address is given a user input such as what TV can I buy if I'm on a budget to retrieve the relevant categories and products so that we have the right info to answer the user's query. So here's the prompts. If you feel to pause the video and read through this in detail, if you wish.

But the prompts specifies a set of instructions. and it actually gives the language model one example of a good output. This is sometimes called a few short or technically one short prompting because we're actually using a user message and a system message to give it one example of a good output. Someone says, I want the most expensive computer yeah, let's just return all the computers because we don't have pricing information. Now let's use this prompt on the customer message, which TV can I buy if I'm on a budget?

And so we're passing in to this, both the prompts, custom message 0 as well as the products and category. This is a information that we have retrieved up top using the utils function. And here it lists out the relevant information to this query, which is on the academy televisions and whole theater systems. This is a list of TVs and whole theater systems that seem relevant. To see how well the prompt is doing, you may evaluate it on a second prompt.

Customer says I need to charge them from a smartphone. It looks like it's correctly retrieving this data, category, smartphone, accessories, and list the relevant products and here's another one. So what computers do you have, and hopefully, you'll retrieve a list of the computers. So here, I have 3 prompts and if you are developing this prompt for the first time, it would be quite reasonable to to have 1 or 2 or 3 examples like this, and to keep on tuning the prompt until it gives appropriate outputs until the prompts is retrieving the relevant products and categories to the customer requests for all of your prompts. All three of them in this example.

And if the prompt had been missing some products or something, then what we would do is probably go back to edit the prompt a few times until it gets it. on all three of these prompts. After you've gotten the system to this point, you might then start running the system in testing. Maybe send it to internal test users or try using it yourself. and just run it for a while to see what happens.

And sometimes, you will run across the prompt that it fails on. So here's an example of a prompt. Tell me about the Smiles, profile, and it fills out camera, also what TVs you have. So when I run it on this prompt, it looks like it's outputting the right data, but it also outputs a bunch of text here. This extra junk.

Yeah. It makes it harder to pause this into a Python list of dictionaries. So we don't like that as outputting this extra chunk. So when you run across one example that the system fails on, then common practice is to just note down that this is a somewhat tricky example So let's add this to our set of examples that we're going to test the system on systematically. And if you keep on running the system for a while longer, maybe it works on most examples.

We did tune the prompt to see examples. So maybe it will work on many examples. But just by chance, you might run across another example where it generates an error. So this customer message for also causes the system to output a junk a bunch of junk text at the end that we don't want. Trying to be helpful to give all this extra text, but we actually don't want this.

And so at this point, you may have run this prompt maybe on hundreds of examples, maybe of test users, but you would just take the example the tricky ones is doing poorly on. And now I have this set of 5 examples indexed from 0 to 4. I have this set of 5 examples that you use to further fine tune the prompts. And in both of these examples, LLM had output a bunch of extra junk text at the end that we don't want. And after a little bit of trial and error, you might decide to modify the prompts as follows.

So here's a new prompt. This is called prompt V Two. But what we did here was we added to the prompt. Do not output any additional text that's not in JSON format. Just to emphasize, please don't output this JSON itself.

and added a second example using the user and assistant message for future prompting, where the user also is the cheapest computer. And in both of the few short examples, we're demonstrating to the system a response where it gives only Jason output. So here's the extra thing that we just added to the problem to not output any additional text that's not in JSON formats, and we use future user 1, future assistant 1, and Future uses 2, future assistant 2, to give it 2 of these few shot prompts. So let me hit shift enter to find that prompt. and you were to go back and manually rerun this prompt on all 5 of the examples of user inputs, including this one that previously had given the broken output, you'll find that it now gives a correct output.

And if you were to go back and rerun this new prompt, this is prompt version v 2 on that customer message example that had results in the broken output with the extra chunk after the JSON output. then this will generate a better output. And I'm not gonna do it here, but I encourage you to pause a video and rerun it yourself on customer message 4 as well on this prompt v Two. See if it also generates the correct output. And, hopefully, it will.

I think it should. And, of course, when you modify the prompts, it's also useful to do a bit of regression testing to make sure that when fixing the incorrect outputs on prompts 3 and 4 it didn't break the output on prompt 0 either. Now you can kinda tell that if I had to copy paste 5 prompts Customer such as 0, 1, 2, 3, and 4 into my Jupyter notebook and run them and then manually look at them to see if they all put in the right categories and products you can kinda do it. I can look at this and go, yep. Cavalry, TV, and health care systems, products.

Yep. Mostly, you got all of them. But it's actually a little bit painful to do this manually, to manually inspect or to look at this output to make sure with your eyes that this is exactly the right output. So when the development set that you're tuning to becomes more than just a small handful of examples, it then becomes useful to start to automate the testing process. So, here is a set of ten examples where I'm specifying 10 customer messages.

So here's a customer message. What TV can I buy from a budget? as well as what's the ideal answer. Think of this as the right answer in the test set or, really, I should say, development set because we're actually tuning to this. And so we've collected here 10 examples index from 0 through 9, where The last one is if the user says, I would like hot tub timer machine, we have no relevant products to that.

I'm really sorry. So the ideal answer is the empty set. And now, if you want to evaluate automatically what the prompt is doing on any of these 10 examples, here is a function to do so. This kind of a long function. Feel free to pause the video and read through it if you wish, but let me just demonstrate what this is actually doing.

So let me print out the customer message for customer message 0. Right? So customer customer messages, which TV can I buy if I'm on a budget? and let's also print out the ideal answer. So the ideal answer is here are all the TVs that the we want to prompt to retrieve.

And let me now call the prompt. This is promptv2 on this customer message that user products and category information. Let's print it out, and then we'll call the eval we'll call the eval responsive ideal option to see how well the response matches the ideal answer. And in this case, it did output the category that we wanted, and it did output the entire list of products, and so this gives you the score of 1.0. just to show you one more example, it turns out that I know it gets it wrong on example 7.

So if I change this from 0 to 7, and run it. This is what it gets, or let me update this to 7 as well. So under this customer message, this is the ideal answer where it should output under gaming consoles and accessories, a list of gaming consoles and accessories. But whereas the response here has 3 outputs it actually should have had 1, 2, 3, 4, 5 outputs, and so it's missing some of the products. So what I would do, if I'm tuning the prompt now is I would then use a for loop to loop over all 10 of the development set examples where we repeatedly pull up the customer message, get the ideal answer the right answer, call the to get a response, evaluate it, and then, yeah, accumulated an average.

And let me just run this. Alright. Alright. So this will take a while to run, but when it's done running, this is a result. We're running through the 10 examples.

It looks like example 7, was wrong, and so the fraction correct level 10 was 90%. Correct. And so if you were to tuned the prompts. You can rerun this to see if the percent correct goes up or down. What you just saw in this notebook is going through steps 1, 2, and 3 of this bulleted list.

And this already gives a pretty good development sets of 10 examples with which to tune and validate the prompts is working. If you needed an additional rig level of rigor, then you now have the software needed to collect a randomly sample sets of maybe a 100 examples with their ideal outputs. and maybe even go beyond that to the rigor of a holdout test set that you don't even look at while you're tuning the prompts. But for a lot of applications, stopping at bullet 3, but there are also survey applications where you could do what you just saw me do and distribute the notebook and and get to a pretty performance system quite quickly with getting the important caveat that if you're working on a safety critical application or an application with this with this nontrivial risk of harm, then, of course, it would be the responsible thing to do to actually get a much larger test set to really verify the performance before you use it anywhere. And so that's it.

I find that the workflow of building applications using prompts is very different than the workflow. building applications using supervised learning, and the pace of iteration feels much faster. And if you have not yet done it before, you might be surprised at how well an evaluation method built on just a few hand curated tricky examples. You think with 10 examples, and this is not statistically valid for almost anything. But you might be surprised when you actually use this procedure, how effective adding a handful, just a handful of tricky examples into development sets might be in terms of helping you and your team get to an effective set of prompts and effective system.

In this video, the OPUS could be evaluated quantitatively as in there was a desired output, and you could tell if it gave this desired output or not. So in the next video, let's take a look at how you can evaluate outputs in that setting where what is the right answer is a bit more ambiguous.
