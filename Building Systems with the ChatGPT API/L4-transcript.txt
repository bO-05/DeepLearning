Speaker 0:

In this section, we'll focus on tasks to process inputs. i e the tasks that take the input and generate a useful output often through a series of steps. It is sometimes important for the model to reason in detail about a problem before answering a specific question. And if you took our previous course, chat GPT prompt engineering for developers, you will have seen a number of examples of this. Sometimes a model might make reasoning errors by rushing to an incorrect conclusion so we can reframe the query to request a series of relevant reasoning steps before the model provides a final answer so that it can think longer and more methodically about the problem.

And in general, we call the strategy of asking the model to reason about a problem in steps, chain of thought reasoning. For some applications, the reasoning process that our model uses to arrive at a final answer would be inappropriate to share with the user. For example, in tutoring applications, we may want to encourage students to work on their own answers but a moral reasoning process about the student solution could reveal the answer to the student. In a monologue is a tactic that can be used to mitigate this. And this is just a fancy way of saying hiding the models reasoning from the user.

The idea of inner monologue is to instruct the model to put parts of the output that are meant to be hidden from the user into a structured format that makes passing them easy. Then before presenting the output to the user, the output is passed and only part of the output is made visible. So remember the classification problem from a previous video where we asked the model to classify a customer query into a primary and secondary category. And based on that classification, we might want to take different instructions. Imagine the customer query had been classified into the product information category.

In the next instructions, we'll want to include information about the products we have available. And so in this case, the classification would have been primary, general inquiry, secondary product information. And so let's dive into an example starting from there. So let's start with our usual setup. So for this inner monologue example, we'll start with our same delimiter as that we've been using, and now let's go through our system message.

And so what we're doing here is asking the model to reason about the answer before coming to its conclusion. So the instruction is follow these steps to answer the customer queries. The customer query will be delimited with 4 hashtags or delimiter. So now we've split this up into steps. So the first step is to decide whether the user is asking a question about a specific product or products.

and a product category doesn't count. Step 2. So if the user is asking about specific products, identify whether the products are in the following list and now we've included a a list of available products. So here we have 5 available products that are all varieties of laptops and these are all made up products. They were actually generated by GPT 4.

In step 3, If the message contains products in the list above list any assumptions that the user is making in their message for example that laptop x is bigger than laptop y or that laptop zed has a 2 year warranty for example. Step 4 is if the user made any assumptions, figure out whether the assumption is true based on your product information, and step 5 is First, politely correct the customers' incorrect assumptions if applicable. Only mention or reference products in the list of 5 available products. as these are the only five products that the store sells and answer the customer in a friendly tone. And these kind of very pedantic instructions are probably unnecessary for a more advanced language model like GPT 4.

And then we'll ask the model to use the following format. So step 1, delimiter, it's reasoning, step 2, delimiter reasoning, and so on. And using the delimiters will mean that it will be easier for later to get just this response to the customer and kind of cut off everything before? So now let's try an example. User message.

So our message is "By how much is the Blue Wave Chromebook more expensive than the tech pro desktop?" So let's take a look at these two products. The Blue Wave Chromebook is 249.99, and the tech pro desktop is actually 999.99. This is not actually true. And so let's see how the model handles this user request.

So we'll format into our messages array, and we'll get our response. and then we'll print it. And so what we're hoping for is that the model takes all of these different steps and realizes that the user has made an incorrect assumption and then follows the final step to politely correct the user. And so within this one prompt, we've actually maintained a number of different complex states that the system could be in. So you know, at any given point, that could be a different output from the previous step, and we would want to do something different.

For example, if the user hadn't made any assumptions in in step 3, and then step 4, we wouldn't actually have any output. So this is a pretty complicated instruction for the model. So let's see if it did it. Right? So step 1, the user is asking a question about specific products.

They're asking about the price difference between these two products. The user assumes that the Blue Wave Chromebook is more expensive than the techbook pro, and this assumption is actually incorrect. it's reasoning through, taking longer to think about the problem. In the same way that human would also take some time to reason about an answer to any given question. The model performs better if it also has time to think.

And so the final response to the user is the BlueWave Chromebook is actually less expensive than the techbook pro. The techbook pro, desktop cost 999.99.99. while the BlueWave Chromebook costs 24999. And so let's see in another example of a user message. And, also, at this point, feel free to pause the video and try your own messages.

So let's format this user message. So the question is, do you sell TVs? And if you remember in our product list, we've only listed different computers. so let's see what the model says. So in this case, step 1, the user is asking if the store sells TVs but TVs are not listed in the available products.

So as you can see, the model then skips to the response to user stat because it realizes that the intermediary steps are not actually necessary. I will say that we did ask for the output in this specific format, so technically, the model hasn't exactly followed our request. Again, more advanced models will be better at doing that. And so in this case, our response to the user is I'm sorry, but we do not sell TVs at the store and then it lists the available products. So, again, feel free to try some of your own responses.

And so now we only really want this part of the response we wouldn't wanna show the earlier parts to the user. So what we can do is actually just cut the string at the last occurrence of this delimiter token or string of 4 hashtags and then only print the final part of the model output. So let's write some code to get only the final part of this string. So we're going to use a try except block to gracefully handle errors in case the model has some kind of unpredictable output and doesn't actually use these these characters. And so we're gonna say our final response is the response, and then we're gonna split the string at the delimiter string.

because we want the final occurrence, we just want to get the last item in the output list, and then we're going to strip any white space. because as you can see there might be white space after the characters. Then we're going to catch any errors. and have a fallback response which is, sorry, I'm having I'm having trouble right now. please try asking another question.

And then let's print our final response. And so as you can see we just cut the string to get this final output. And so this is what we would show to the user if we were building this into an application. And overall, I just want to call out this prompt might be slightly convoluted for this task. You might not actually need all of these intermediate steps.

And so why don't you try and see if you can find an easier way do the same task in your own prompt. And in general, finding the optimal trade off in prompt complexity requires some experimentation. So definitely good to try a number of different prompts before deciding to use 1. And in the next video, we'll learn another strategy to handle complex tasks by splitting these complex tasks into a series of simpler subtasks rather than trying to do the whole task in one prompt.
